{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe9vkEvFABbN"
   },
   "source": [
    "# How to Train YOLOv10 Object Detection on a Custom Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/pdf/2405.14458.pdf)\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/THU-MIG/yolov10)\n",
    "[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/YOLO-ARENA)\n",
    "\n",
    "YOLOv10 is a new generation in the YOLO series for real-time end-to-end object detection. It aims to improve both the performance and efficiency of YOLOs by eliminating the need for non-maximum suppression (NMS) and optimizing model architecture comprehensively. This advancement reduces computational overhead, enhancing both efficiency and capability. YOLOv10 shows state-of-the-art performance and efficiency, with YOLOv10-S being 1.8 times faster than RT-DETR-R18 and having significantly fewer parameters and FLOPs. Additionally, YOLOv10-B demonstrates 46% less latency and 25% fewer parameters compared to YOLOv9-C while maintaining the same performance.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/nrl-ai/anylearning/3210cd0c214d3a465a61c3fdfe78324d58d17a1f/docs/yolov10/yolov10_latency.svg\" width=48%>\n",
    "  <img src=\"https://raw.githubusercontent.com/nrl-ai/anylearning/3210cd0c214d3a465a61c3fdfe78324d58d17a1f/docs/yolov10/yolov10_params.svg\" width=48%> <br>\n",
    "  Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3EO_2zNChu"
   },
   "source": [
    "## Install YOLOv10 + Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdSMcABDNKW-",
    "outputId": "c80e35a5-3055-4bf1-d65b-bbd5f75a5594"
   },
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/THU-MIG/yolov10.git\n",
    "%pip install huggingface_hub anylearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "9311d67a-2c89-4bab-f16d-a481cb470be7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WORKSPACE = os.path.abspath(\"./workspace\")\n",
    "print(\"Workspace:\", WORKSPACE)\n",
    "\n",
    "# Copy the dataset into WORKSPACE/raw_dataset\n",
    "# This is the dataset in format of AnyLabeling\n",
    "# The structure should be:\n",
    "# WORKSPACE/raw_dataset\n",
    "# ├── 0000.jpg\n",
    "# ├── 0000.json\n",
    "# ├── 0001.jpg\n",
    "# ├── 0001.json\n",
    "# ...\n",
    "RAW_DATA_ROOT = os.path.join(WORKSPACE, \"raw_dataset\")\n",
    "\n",
    "# This is the dataset that you will use for training\n",
    "DATA_ROOT = os.path.join(WORKSPACE, \"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into the format for YOLOv10\n",
    "# The structure should be:\n",
    "# WORKSPACE/dataset\n",
    "# ├── train\n",
    "# │   ├── images\n",
    "# │   │   ├── 0000.jpg\n",
    "# │   │   ├── 0001.jpg\n",
    "# │   │   ...\n",
    "# │   └── labels\n",
    "# │       ├── 0000.txt\n",
    "# │       ├── 0001.txt\n",
    "# │       ...\n",
    "# ├── val\n",
    "# │   ├── ...\n",
    "# └── test\n",
    "#     ├── ...\n",
    "!python -m anylearning.converter --json_dir {RAW_DATA_ROOT} --val_size 0.1 --test_size 0.1 --output_dir {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMEtcxdshoEC"
   },
   "source": [
    "## Download pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF1nAW3Dri83"
   },
   "source": [
    "**NOTE:** YOLOv10 provides weight files pre-trained on the COCO dataset in various sizes. Let's download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2l67kw8xiYPX",
    "outputId": "623d812e-d462-4346-c992-0218854b52e9"
   },
   "outputs": [],
   "source": [
    "!mkdir -p {WORKSPACE}/weights\n",
    "!wget -P {WORKSPACE}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10n.pt\n",
    "# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt\n",
    "# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt\n",
    "# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt\n",
    "# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10x.pt\n",
    "# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10l.pt\n",
    "!ls -lh {WORKSPACE}/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUjFBKKqXa-u"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2YkphuiaE7_",
    "outputId": "6145b761-5a99-4c8a-d17c-db03baa115b7"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "data_path = os.path.join(WORKSPACE, \"dataset/dataset.yaml\")\n",
    "n_epochs = 10\n",
    "bs = 16\n",
    "gpu_id = 0\n",
    "rng = 0\n",
    "verbose = True\n",
    "validate = True\n",
    "\n",
    "model = YOLO(os.path.join(WORKSPACE, \"weights\", \"yolov10n.pt\"))\n",
    "results = model.train(\n",
    "    data=data_path,\n",
    "    epochs=n_epochs,\n",
    "    batch=bs,\n",
    "    device=gpu_id,\n",
    "    verbose=verbose,\n",
    "    seed=rng,\n",
    "    val=validate\n",
    ")\n",
    "\n",
    "# After this step, you can find the trained model in ./runs/train/exp/weights/best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh6h0MOEy2WX"
   },
   "source": [
    "## Inference with Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNjsAO8m08ti"
   },
   "source": [
    "Load the model and make predictions on a sample image from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY1ajwSzyXCE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "from ultralytics import YOLOv10\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from anylearning.utils import list_images\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the best model\n",
    "# Check the result of the training inside the ./runs/detect/train* folder\n",
    "model = YOLOv10(\"runs/detect/train/weights/best.pt\")\n",
    "\n",
    "test_images = list(list_images(os.path.join(DATA_ROOT, \"val/images\")))\n",
    "\n",
    "# Randomly select an image\n",
    "test_image = random.choice(test_images)\n",
    "\n",
    "# Load the image and predict\n",
    "# Convert the image to RGB format as OpenCV loads images in BGR format\n",
    "img = cv2.imread(test_image)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "results = model.predict(img)\n",
    "\n",
    "# Process results list\n",
    "result = results[0]  # first image results\n",
    "boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "masks = result.masks  # Masks object for segmentation masks outputs\n",
    "keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "probs = result.probs  # Probs object for classification outputs\n",
    "obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "image = result.plot()  # plot predictions\n",
    "\n",
    "imshow(image)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
